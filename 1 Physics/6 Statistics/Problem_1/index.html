<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<link href="../../../img/favicon.ico" rel="shortcut icon"/>
<title>Statistics - Physics and Mathematics</title>
<link href="../../../css/theme.css" rel="stylesheet"/>
<link href="../../../css/theme_extra.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" rel="stylesheet"/>
<script>
        // Current page data
        var mkdocs_page_name = "Statistics";
        var mkdocs_page_input_path = "1 Physics/6 Statistics/Problem_1.md";
        var mkdocs_page_url = null;
      </script>
<!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/rust.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
<script>hljs.highlightAll();</script>
</head>
<body class="wy-body-for-nav" role="document">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side stickynav" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../../.."> Physics and Mathematics
        </a><div role="search">
<form action="../../../search.html" class="wy-form" id="rtd-search-form" method="get">
<input aria-label="Search docs" name="q" placeholder="Search docs" title="Type search term here" type="text"/>
</form>
</div>
</div>
<div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../..">Introduction</a>
</li>
</ul>
<p class="caption"><span class="caption-text">1 Physics</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal">1 Mechanics</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../1%20Mechanics/Problem_1/">Problem 1</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../1%20Mechanics/Problem_2/">Problem 2</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">2 Gravity</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../2%20Gravity/Problem_1/">Problem 1</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../2%20Gravity/Problem_2/">Problem 2</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../2%20Gravity/Problem_3/">Problem 3</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">3 Waves</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../3%20Waves/Problem_1/">Problem 1</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">4 Electromagnetism</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../4%20Electromagnetism/Problem_1/">Electromagnetism</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">5 Circuits</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../5%20Circuits/Problem_1/">Problem 1</a>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal current">6 Statistics</a>
<ul class="current">
<li class="toctree-l2 current"><a class="reference internal current" href="#">Statistics</a>
<ul class="current">
<li class="toctree-l3"><a class="reference internal" href="#problem-1-exploring-the-central-limit-theorem-through-simulations">Problem 1: Exploring the Central Limit Theorem through Simulations</a>
<ul>
<li class="toctree-l4"><a class="reference internal" href="#motivation">Motivation</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#theoretical-framework">Theoretical Framework</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#implementation">Implementation</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#21-uniform-distribution">2.1 Uniform Distribution</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#22-exponential-distribution">2.2 Exponential Distribution</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#23-binomial-distribution">2.3 Binomial Distribution</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#24-gamma-distribution">2.4 Gamma Distribution</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#25-comparing-convergence-rates">2.5 Comparing Convergence Rates</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#3-parameter-exploration">3. Parameter Exploration</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#4-practical-applications">4. Practical Applications</a>
</li>
<li class="toctree-l4"><a class="reference internal" href="#5-conclusion">5. Conclusion</a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Problem_2/">Estimating π using Monte Carlo Methods</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">7 Measurements</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../7%20Measurements/Problem_1/">Problem 1: Measuring Earth's Gravitational Acceleration with a Pendulum</a>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">2 Mathematics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../2%20Mathematics/1%20Linear_algebra/">Linear Algebra</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../2%20Mathematics/2%20Analytic_geometry/">Analytic geometry</a>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../2%20Mathematics/3%20Calculus/">Calculus</a>
</li>
</ul>
<p class="caption"><span class="caption-text">3 Discret Mathematics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal">1 Set Theory and ...</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/1%20Set%20Theory%20and%20.../_02%20Set_Theory/">Set Theory</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/1%20Set%20Theory%20and%20.../_03%20Relations/">Relations</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/1%20Set%20Theory%20and%20.../_04%20Functions/">Functions</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">2 Number Theory and ...</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/2%20Number%20Theory%20and%20.../_05%20Combinatorics/">Combinatorics</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/2%20Number%20Theory%20and%20.../_08%20Number_Theory/">Number Theory</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">3 Recurrence and ...</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/3%20Recurrence%20and%20.../_06%20Sequences_and_Series/">Sequences and Series</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/3%20Recurrence%20and%20.../_07%20Induction/">Induction</a>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/3%20Recurrence%20and%20.../_09%20Recurrence/">Recurrence</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">4 Graph Theory and ...</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/4%20Graph%20Theory%20and%20.../_10%20Graph_Theory/">Graph Theory</a>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal">5 Logic</a>
<ul>
<li class="toctree-l2"><a class="reference internal" href="../../../3%20Discret_Mathematics/5%20Logic/_01%20Logic/">Logic</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="Mobile navigation menu" class="wy-nav-top" role="navigation">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../../..">Physics and Mathematics</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content"><div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a aria-label="Docs" class="icon icon-home" href="../../.."></a></li>
<li class="breadcrumb-item">1 Physics</li>
<li class="breadcrumb-item">6 Statistics</li>
<li class="breadcrumb-item active">Statistics</li>
<li class="wy-breadcrumbs-aside">
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div class="section" itemprop="articleBody">
<h1 id="statistics">Statistics</h1>
<h2 id="problem-1-exploring-the-central-limit-theorem-through-simulations">Problem 1: Exploring the Central Limit Theorem through Simulations</h2>
<h3 id="motivation">Motivation</h3>
<p>The Central Limit Theorem (CLT) is a cornerstone of probability and statistics, stating that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population's original distribution. This theorem is fundamental to many statistical methods and has wide-ranging applications in fields from finance to quality control.</p>
<p>While the CLT can be proven mathematically, simulations provide an intuitive and hands-on way to observe this phenomenon in action. Through computational experiments, we can visualize how sample means from various distributions converge to normality as sample size increases, deepening our understanding of this important statistical principle.</p>
<h3 id="theoretical-framework">Theoretical Framework</h3>
<h4 id="11-the-central-limit-theorem">1.1 The Central Limit Theorem</h4>
<p>The Central Limit Theorem states that:</p>
<p>Given a population with mean <span class="arithmatex">\(\mu\)</span> and standard deviation <span class="arithmatex">\(\sigma\)</span>, if we take random samples of size <span class="arithmatex">\(n\)</span> from this population and calculate the sample mean <span class="arithmatex">\(\bar{X}\)</span> for each sample, the distribution of these sample means will:</p>
<ol>
<li>Have a mean equal to the population mean: <span class="arithmatex">\(\mu_{\bar{X}} = \mu\)</span></li>
<li>Have a standard deviation equal to the population standard deviation divided by the square root of the sample size: <span class="arithmatex">\(\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}\)</span></li>
<li>Approach a normal distribution as <span class="arithmatex">\(n\)</span> increases, regardless of the shape of the original population distribution</li>
</ol>
<p>Mathematically, for large <span class="arithmatex">\(n\)</span>:</p>
<div class="arithmatex">\[\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\]</div>
<p>Or, the standardized sample mean:</p>
<div class="arithmatex">\[Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)\]</div>
<h4 id="12-conditions-for-the-clt">1.2 Conditions for the CLT</h4>
<p>While the CLT is remarkably robust, certain conditions affect how quickly the sampling distribution converges to normality:</p>
<ol>
<li><strong>Sample Size</strong>: Larger samples lead to faster convergence to normality</li>
<li><strong>Population Distribution</strong>: Some distributions (like the uniform) converge more quickly than others (like the exponential or highly skewed distributions)</li>
<li><strong>Independence</strong>: The sampled observations should be independent</li>
<li><strong>Finite Variance</strong>: The population should have a finite variance (though there are extensions of the CLT for infinite variance cases)</li>
</ol>
<h3 id="implementation">Implementation</h3>
<p>Let's implement simulations to explore the Central Limit Theorem with different population distributions and sample sizes.</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pandas as pd

# Set style for better visualizations
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("viridis")

# Function to generate population data
def generate_population(distribution_type, size=10000, **params):
    """
    Generate a population from a specified distribution.

    Parameters:
    -----------
    distribution_type : str
        Type of distribution ('uniform', 'exponential', 'binomial', 'normal', 'gamma')
    size : int
        Size of the population
    **params : dict
        Parameters for the distribution

    Returns:
    --------
    array
        Population data
    """
    if distribution_type == 'uniform':
        return np.random.uniform(params.get('low', 0), params.get('high', 1), size)
    elif distribution_type == 'exponential':
        return np.random.exponential(params.get('scale', 1), size)
    elif distribution_type == 'binomial':
        return np.random.binomial(params.get('n', 10), params.get('p', 0.5), size)
    elif distribution_type == 'normal':
        return np.random.normal(params.get('mean', 0), params.get('std', 1), size)
    elif distribution_type == 'gamma':
        return np.random.gamma(params.get('shape', 2), params.get('scale', 1), size)
    else:
        raise ValueError(f"Distribution type '{distribution_type}' not supported")

# Function to simulate sampling distribution
def simulate_sampling_distribution(population, sample_size, n_samples=1000):
    """
    Simulate the sampling distribution of the mean.

    Parameters:
    -----------
    population : array
        Population data
    sample_size : int
        Size of each sample
    n_samples : int
        Number of samples to draw

    Returns:
    --------
    array
        Sample means
    """
    sample_means = []
    for _ in range(n_samples):
        sample = np.random.choice(population, size=sample_size, replace=True)
        sample_means.append(np.mean(sample))
    return np.array(sample_means)

# Function to plot population and sampling distributions
def plot_distributions(population, sample_means_dict, distribution_name):
    """
    Plot the population distribution and sampling distributions for different sample sizes.

    Parameters:
    -----------
    population : array
        Population data
    sample_means_dict : dict
        Dictionary with sample sizes as keys and sample means as values
    distribution_name : str
        Name of the distribution for the title
    """
    # Calculate population parameters
    pop_mean = np.mean(population)
    pop_std = np.std(population)

    # Create figure with subplots
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle(f'Central Limit Theorem: {distribution_name} Distribution', fontsize=16)

    # Plot population distribution
    sns.histplot(population, kde=True, ax=axes[0, 0], bins=50)
    axes[0, 0].axvline(pop_mean, color='red', linestyle='--', label=f'Mean: {pop_mean:.2f}')
    axes[0, 0].set_title('Population Distribution')
    axes[0, 0].legend()

    # Plot sampling distributions for different sample sizes
    for i, (sample_size, sample_means) in enumerate(sample_means_dict.items(), 1):
        row = i // 3
        col = i % 3

        # Calculate theoretical parameters
        theo_mean = pop_mean
        theo_std = pop_std / np.sqrt(sample_size)

        # Plot histogram of sample means
        sns.histplot(sample_means, kde=True, ax=axes[row, col], bins=50)
        axes[row, col].axvline(theo_mean, color='red', linestyle='--', 
                              label=f'Theo. Mean: {theo_mean:.2f}')

        # Add normal distribution curve for comparison
        x = np.linspace(min(sample_means), max(sample_means), 100)
        y = stats.norm.pdf(x, theo_mean, theo_std)
        axes[row, col].plot(x, y * len(sample_means) * (max(sample_means) - min(sample_means)) / 50, 
                           'r-', linewidth=2, label='Normal PDF')

        axes[row, col].set_title(f'Sample Size: {sample_size}')
        axes[row, col].legend()

    # Remove empty subplot if any
    if len(sample_means_dict) &lt; 5:
        axes[1, 2].remove()

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(f'clt_{distribution_name.lower()}.png', dpi=300, bbox_inches='tight')
    plt.show()

# Function to analyze convergence to normality
def analyze_normality(sample_means_dict, distribution_name):
    """
    Analyze how quickly the sampling distribution converges to normality.

    Parameters:
    -----------
    sample_means_dict : dict
        Dictionary with sample sizes as keys and sample means as values
    distribution_name : str
        Name of the distribution for the title
    """
    # Calculate Shapiro-Wilk test p-values for each sample size
    p_values = {}
    for sample_size, sample_means in sample_means_dict.items():
        _, p_value = stats.shapiro(sample_means)
        p_values[sample_size] = p_value

    # Create a bar plot of p-values
    plt.figure(figsize=(10, 6))
    plt.bar(p_values.keys(), p_values.values(), color='skyblue')
    plt.axhline(y=0.05, color='red', linestyle='--', label='α = 0.05')
    plt.xlabel('Sample Size')
    plt.ylabel('Shapiro-Wilk Test p-value')
    plt.title(f'Normality Test p-values: {distribution_name} Distribution')
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Add text annotations
    for i, (sample_size, p_value) in enumerate(p_values.items()):
        plt.text(sample_size, p_value + 0.01, f'{p_value:.4f}', 
                 ha='center', va='bottom')

    plt.tight_layout()
    plt.savefig(f'normality_test_{distribution_name.lower()}.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Print interpretation
    print(f"\nNormality Analysis for {distribution_name} Distribution:")
    print("-" * 50)
    for sample_size, p_value in p_values.items():
        if p_value &gt; 0.05:
            print(f"Sample size {sample_size}: p-value = {p_value:.4f} &gt; 0.05")
            print(f"  → Sampling distribution is approximately normal")
        else:
            print(f"Sample size {sample_size}: p-value = {p_value:.4f} ≤ 0.05")
            print(f"  → Sampling distribution deviates from normality")
    print("-" * 50)

# Function to compare convergence rates across distributions
def compare_convergence_rates(distributions_dict):
    """
    Compare how quickly different distributions converge to normality.

    Parameters:
    -----------
    distributions_dict : dict
        Dictionary with distribution names as keys and sample means dictionaries as values
    """
    # Calculate Shapiro-Wilk test p-values for each distribution and sample size
    results = []
    for dist_name, sample_means_dict in distributions_dict.items():
        for sample_size, sample_means in sample_means_dict.items():
            _, p_value = stats.shapiro(sample_means)
            results.append({
                'Distribution': dist_name,
                'Sample Size': sample_size,
                'p-value': p_value
            })

    # Convert to DataFrame
    df = pd.DataFrame(results)

    # Create a heatmap
    pivot_df = df.pivot(index='Distribution', columns='Sample Size', values='p-value')

    plt.figure(figsize=(12, 8))
    sns.heatmap(pivot_df, annot=True, cmap='RdYlGn_r', vmin=0, vmax=1, 
                fmt='.4f', linewidths=.5, cbar_kws={'label': 'p-value'})
    plt.title('Convergence to Normality: Shapiro-Wilk Test p-values')
    plt.tight_layout()
    plt.savefig('convergence_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Print interpretation
    print("\nConvergence Rate Comparison:")
    print("-" * 50)
    for dist_name in distributions_dict.keys():
        dist_df = df[df['Distribution'] == dist_name]
        min_size = dist_df[dist_df['p-value'] &gt; 0.05]['Sample Size'].min()
        if pd.isna(min_size):
            print(f"{dist_name}: Does not converge to normality within the tested sample sizes")
        else:
            print(f"{dist_name}: Converges to normality at sample size {min_size}")
    print("-" * 50)
</code></pre>
<h3 id="21-uniform-distribution">2.1 Uniform Distribution</h3>
<p>Let's start with a uniform distribution, which has a rectangular shape and is one of the simplest distributions to understand.</p>
<pre><code class="language-python"># Generate uniform population
uniform_pop = generate_population('uniform', size=10000, low=0, high=1)

# Simulate sampling distributions for different sample sizes
sample_sizes = [5, 10, 30, 50]
uniform_sample_means = {}
for size in sample_sizes:
    uniform_sample_means[size] = simulate_sampling_distribution(uniform_pop, size, n_samples=1000)

# Plot distributions
plot_distributions(uniform_pop, uniform_sample_means, 'Uniform')

# Analyze convergence to normality
analyze_normality(uniform_sample_means, 'Uniform')
</code></pre>
<p><img alt="Uniform Distribution CLT" src="../clt_uniform.png"/>
<img alt="Uniform Normality Test" src="../normality_test_uniform.png"/></p>
<p>The uniform distribution demonstrates rapid convergence to normality. Even with small sample sizes, the sampling distribution of the mean quickly approaches a normal distribution. This is because the uniform distribution is symmetric and has no extreme values or heavy tails.</p>
<h3 id="22-exponential-distribution">2.2 Exponential Distribution</h3>
<p>Next, let's examine the exponential distribution, which is skewed and has a heavy tail.</p>
<pre><code class="language-python"># Generate exponential population
exp_pop = generate_population('exponential', size=10000, scale=1)

# Simulate sampling distributions for different sample sizes
exp_sample_means = {}
for size in sample_sizes:
    exp_sample_means[size] = simulate_sampling_distribution(exp_pop, size, n_samples=1000)

# Plot distributions
plot_distributions(exp_pop, exp_sample_means, 'Exponential')

# Analyze convergence to normality
analyze_normality(exp_sample_means, 'Exponential')
</code></pre>
<p><img alt="Exponential Distribution CLT" src="../clt_exponential.png"/>
<img alt="Exponential Normality Test" src="../normality_test_exponential.png"/></p>
<p>The exponential distribution, being highly skewed, requires larger sample sizes to achieve normality in the sampling distribution. With small sample sizes, the sampling distribution retains some of the skewness of the original distribution. As the sample size increases, the distribution of sample means becomes more symmetric and bell-shaped.</p>
<h3 id="23-binomial-distribution">2.3 Binomial Distribution</h3>
<p>The binomial distribution is discrete and can be symmetric or skewed depending on the probability parameter.</p>
<pre><code class="language-python"># Generate binomial population
binom_pop = generate_population('binomial', size=10000, n=10, p=0.3)

# Simulate sampling distributions for different sample sizes
binom_sample_means = {}
for size in sample_sizes:
    binom_sample_means[size] = simulate_sampling_distribution(binom_pop, size, n_samples=1000)

# Plot distributions
plot_distributions(binom_pop, binom_sample_means, 'Binomial')

# Analyze convergence to normality
analyze_normality(binom_sample_means, 'Binomial')
</code></pre>
<p><img alt="Binomial Distribution CLT" src="../clt_binomial.png"/>
<img alt="Binomial Normality Test" src="../normality_test_binomial.png"/></p>
<p>The binomial distribution, being discrete, shows a step-like pattern in small samples. As the sample size increases, the sampling distribution becomes smoother and more closely approximates a normal distribution. The rate of convergence depends on the probability parameter p; distributions with p closer to 0.5 converge more quickly.</p>
<h3 id="24-gamma-distribution">2.4 Gamma Distribution</h3>
<p>The gamma distribution is another example of a skewed distribution with varying degrees of skewness depending on its shape parameter.</p>
<pre><code class="language-python"># Generate gamma population
gamma_pop = generate_population('gamma', size=10000, shape=2, scale=1)

# Simulate sampling distributions for different sample sizes
gamma_sample_means = {}
for size in sample_sizes:
    gamma_sample_means[size] = simulate_sampling_distribution(gamma_pop, size, n_samples=1000)

# Plot distributions
plot_distributions(gamma_pop, gamma_sample_means, 'Gamma')

# Analyze convergence to normality
analyze_normality(gamma_sample_means, 'Gamma')
</code></pre>
<p><img alt="Gamma Distribution CLT" src="../clt_gamma.png"/>
<img alt="Gamma Normality Test" src="../normality_test_gamma.png"/></p>
<p>The gamma distribution, with its moderate skewness, shows intermediate convergence behavior compared to the uniform and exponential distributions. With small sample sizes, the sampling distribution retains some skewness, but as the sample size increases, it approaches a normal distribution.</p>
<h3 id="25-comparing-convergence-rates">2.5 Comparing Convergence Rates</h3>
<p>Let's compare how quickly different distributions converge to normality.</p>
<pre><code class="language-python"># Compare convergence rates
distributions = {
    'Uniform': uniform_sample_means,
    'Exponential': exp_sample_means,
    'Binomial': binom_sample_means,
    'Gamma': gamma_sample_means
}

compare_convergence_rates(distributions)
</code></pre>
<p><img alt="Convergence Comparison" src="../convergence_comparison.png"/></p>
<p>This comparison reveals that:</p>
<ol>
<li>The <strong>uniform distribution</strong> converges most quickly to normality, requiring only small sample sizes.</li>
<li>The <strong>binomial distribution</strong> converges relatively quickly, especially with p close to 0.5.</li>
<li>The <strong>gamma distribution</strong> requires moderate sample sizes to achieve normality.</li>
<li>The <strong>exponential distribution</strong>, being highly skewed, requires the largest sample sizes to converge to normality.</li>
</ol>
<h3 id="3-parameter-exploration">3. Parameter Exploration</h3>
<p>Let's explore how different parameters of the population distributions affect the convergence to normality.</p>
<h4 id="31-effect-of-skewness-in-gamma-distribution">3.1 Effect of Skewness in Gamma Distribution</h4>
<pre><code class="language-python"># Generate gamma populations with different shape parameters
gamma_shapes = [0.5, 1, 2, 5]
gamma_pops = {}
for shape in gamma_shapes:
    gamma_pops[shape] = generate_population('gamma', size=10000, shape=shape, scale=1)

# Simulate sampling distributions for a fixed sample size
sample_size = 30
gamma_sample_means_by_shape = {}
for shape, pop in gamma_pops.items():
    gamma_sample_means_by_shape[shape] = simulate_sampling_distribution(pop, sample_size, n_samples=1000)

# Plot distributions
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle(f'Effect of Skewness on Convergence: Gamma Distribution (n={sample_size})', fontsize=16)

for i, (shape, sample_means) in enumerate(gamma_sample_means_by_shape.items()):
    row = i // 2
    col = i % 2

    # Calculate theoretical parameters
    pop = gamma_pops[shape]
    pop_mean = np.mean(pop)
    pop_std = np.std(pop)
    theo_mean = pop_mean
    theo_std = pop_std / np.sqrt(sample_size)

    # Plot histogram of sample means
    sns.histplot(sample_means, kde=True, ax=axes[row, col], bins=50)
    axes[row, col].axvline(theo_mean, color='red', linestyle='--', 
                          label=f'Theo. Mean: {theo_mean:.2f}')

    # Add normal distribution curve for comparison
    x = np.linspace(min(sample_means), max(sample_means), 100)
    y = stats.norm.pdf(x, theo_mean, theo_std)
    axes[row, col].plot(x, y * len(sample_means) * (max(sample_means) - min(sample_means)) / 50, 
                       'r-', linewidth=2, label='Normal PDF')

    # Calculate skewness
    skewness = stats.skew(gamma_pops[shape])

    axes[row, col].set_title(f'Shape = {shape} (Skewness = {skewness:.2f})')
    axes[row, col].legend()

    # Add Shapiro-Wilk test p-value
    _, p_value = stats.shapiro(sample_means)
    axes[row, col].text(0.05, 0.95, f'p-value: {p_value:.4f}', 
                       transform=axes[row, col].transAxes, 
                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.savefig('gamma_skewness_effect.png', dpi=300, bbox_inches='tight')
plt.show()
</code></pre>
<p><img alt="Effect of Skewness in Gamma Distribution" src="../gamma_skewness_effect.png"/></p>
<p>This visualization shows that as the shape parameter of the gamma distribution increases (reducing skewness), the sampling distribution converges more quickly to normality. With a shape parameter of 0.5 (highly skewed), the sampling distribution still shows some deviation from normality even with a sample size of 30. As the shape parameter increases to 5 (less skewed), the sampling distribution closely approximates a normal distribution.</p>
<h4 id="32-effect-of-sample-size-on-standard-error">3.2 Effect of Sample Size on Standard Error</h4>
<p>The standard error of the mean decreases as the sample size increases, following the relationship <span class="arithmatex">\(\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}\)</span>.</p>
<pre><code class="language-python"># Calculate standard errors for different sample sizes
standard_errors = {}
for dist_name, sample_means_dict in distributions.items():
    standard_errors[dist_name] = {}
    for sample_size, sample_means in sample_means_dict.items():
        standard_errors[dist_name][sample_size] = np.std(sample_means)

# Plot standard errors
plt.figure(figsize=(12, 8))
for dist_name, errors in standard_errors.items():
    plt.plot(list(errors.keys()), list(errors.values()), marker='o', label=dist_name)

# Add theoretical curve
sample_sizes = np.array(list(standard_errors['Uniform'].keys()))
theoretical_se = np.std(uniform_pop) / np.sqrt(sample_sizes)
plt.plot(sample_sizes, theoretical_se, 'k--', label='Theoretical (1/√n)')

plt.xlabel('Sample Size')
plt.ylabel('Standard Error')
plt.title('Standard Error vs. Sample Size')
plt.legend()
plt.grid(True)
plt.savefig('standard_error_vs_sample_size.png', dpi=300, bbox_inches='tight')
plt.show()
</code></pre>
<p><img alt="Standard Error vs. Sample Size" src="../standard_error_vs_sample_size.png"/></p>
<p>This plot demonstrates that the standard error decreases as the square root of the sample size increases, following the theoretical relationship <span class="arithmatex">\(\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}\)</span>. This is a key property of the Central Limit Theorem and has important implications for statistical inference.</p>
<h3 id="4-practical-applications">4. Practical Applications</h3>
<p>The Central Limit Theorem has numerous practical applications in statistics and data science:</p>
<h4 id="41-confidence-intervals">4.1 Confidence Intervals</h4>
<p>The CLT allows us to construct confidence intervals for population parameters, even when the population distribution is unknown.</p>
<pre><code class="language-python"># Demonstrate confidence intervals using the CLT
def demonstrate_confidence_intervals(population, sample_size, n_samples=100, confidence_level=0.95):
    """
    Demonstrate confidence intervals using the CLT.

    Parameters:
    -----------
    population : array
        Population data
    sample_size : int
        Size of each sample
    n_samples : int
        Number of samples to draw
    confidence_level : float
        Confidence level (e.g., 0.95 for 95% confidence)
    """
    # Calculate population mean
    pop_mean = np.mean(population)

    # Generate samples and calculate confidence intervals
    samples = []
    sample_means = []
    sample_stds = []
    ci_lower = []
    ci_upper = []

    for _ in range(n_samples):
        sample = np.random.choice(population, size=sample_size, replace=True)
        samples.append(sample)
        sample_means.append(np.mean(sample))
        sample_stds.append(np.std(sample))

        # Calculate confidence interval
        z_score = stats.norm.ppf((1 + confidence_level) / 2)
        margin_of_error = z_score * sample_stds[-1] / np.sqrt(sample_size)
        ci_lower.append(sample_means[-1] - margin_of_error)
        ci_upper.append(sample_means[-1] + margin_of_error)

    # Plot confidence intervals
    plt.figure(figsize=(12, 8))

    # Plot sample means
    plt.scatter(range(n_samples), sample_means, color='blue', label='Sample Mean')

    # Plot confidence intervals
    for i in range(n_samples):
        if ci_lower[i] &lt;= pop_mean &lt;= ci_upper[i]:
            plt.plot([i, i], [ci_lower[i], ci_upper[i]], 'g-', linewidth=2)
        else:
            plt.plot([i, i], [ci_lower[i], ci_upper[i]], 'r-', linewidth=2)

    # Plot population mean
    plt.axhline(y=pop_mean, color='black', linestyle='--', label='Population Mean')

    plt.xlabel('Sample Number')
    plt.ylabel('Value')
    plt.title(f'{confidence_level*100}% Confidence Intervals (n={sample_size})')
    plt.legend()
    plt.grid(True)

    # Calculate coverage rate
    coverage = sum(1 for i in range(n_samples) if ci_lower[i] &lt;= pop_mean &lt;= ci_upper[i]) / n_samples
    plt.text(0.05, 0.05, f'Coverage Rate: {coverage:.2%}', 
             transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8))

    plt.tight_layout()
    plt.savefig(f'confidence_intervals_n{sample_size}.png', dpi=300, bbox_inches='tight')
    plt.show()

    return coverage

# Demonstrate confidence intervals for different sample sizes
sample_sizes = [5, 30, 100]
coverages = {}

for size in sample_sizes:
    print(f"\nDemonstrating confidence intervals with sample size {size}:")
    coverage = demonstrate_confidence_intervals(uniform_pop, size)
    coverages[size] = coverage
    print(f"Coverage rate: {coverage:.2%}")
</code></pre>
<p><img alt="Confidence Intervals with n=5" src="../confidence_intervals_n5.png"/>
<img alt="Confidence Intervals with n=30" src="../confidence_intervals_n30.png"/>
<img alt="Confidence Intervals with n=100" src="../confidence_intervals_n100.png"/></p>
<p>This demonstration shows how confidence intervals become more reliable as the sample size increases. With small sample sizes, the intervals may not capture the true population mean as frequently as expected. As the sample size increases, the coverage rate approaches the nominal confidence level (e.g., 95%).</p>
<h4 id="42-quality-control">4.2 Quality Control</h4>
<p>In manufacturing, the CLT is used to monitor product quality through control charts.</p>
<pre><code class="language-python"># Demonstrate control charts using the CLT
def demonstrate_control_charts(population, sample_size, n_samples=30):
    """
    Demonstrate control charts using the CLT.

    Parameters:
    -----------
    population : array
        Population data
    sample_size : int
        Size of each sample
    n_samples : int
        Number of samples to draw
    """
    # Calculate population parameters
    pop_mean = np.mean(population)
    pop_std = np.std(population)

    # Generate samples
    samples = []
    sample_means = []
    sample_stds = []

    for _ in range(n_samples):
        sample = np.random.choice(population, size=sample_size, replace=True)
        samples.append(sample)
        sample_means.append(np.mean(sample))
        sample_stds.append(np.std(sample))

    # Calculate control limits
    x_bar = np.mean(sample_means)
    s_bar = np.mean(sample_stds)

    # Constants for control limits (for n=5)
    A3 = 1.427  # For s chart
    B3 = 0  # Lower limit for s chart
    B4 = 2.089  # Upper limit for s chart

    # X-bar chart limits
    x_bar_ucl = x_bar + A3 * s_bar
    x_bar_lcl = x_bar - A3 * s_bar

    # S chart limits
    s_ucl = B4 * s_bar
    s_lcl = B3 * s_bar

    # Plot control charts
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # X-bar chart
    ax1.plot(range(1, n_samples+1), sample_means, 'bo-')
    ax1.axhline(y=x_bar, color='g', linestyle='-', label='Center Line')
    ax1.axhline(y=x_bar_ucl, color='r', linestyle='--', label='UCL')
    ax1.axhline(y=x_bar_lcl, color='r', linestyle='--', label='LCL')
    ax1.axhline(y=pop_mean, color='k', linestyle=':', label='Population Mean')

    ax1.set_xlabel('Sample Number')
    ax1.set_ylabel('Sample Mean')
    ax1.set_title('X-bar Control Chart')
    ax1.legend()
    ax1.grid(True)

    # S chart
    ax2.plot(range(1, n_samples+1), sample_stds, 'ro-')
    ax2.axhline(y=s_bar, color='g', linestyle='-', label='Center Line')
    ax2.axhline(y=s_ucl, color='r', linestyle='--', label='UCL')
    ax2.axhline(y=s_lcl, color='r', linestyle='--', label='LCL')
    ax2.axhline(y=pop_std, color='k', linestyle=':', label='Population Std Dev')

    ax2.set_xlabel('Sample Number')
    ax2.set_ylabel('Sample Standard Deviation')
    ax2.set_title('S Control Chart')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig('control_charts.png', dpi=300, bbox_inches='tight')
    plt.show()

# Demonstrate control charts
demonstrate_control_charts(uniform_pop, sample_size=5)
</code></pre>
<p><img alt="Control Charts" src="../control_charts.png"/></p>
<p>Control charts are used in quality control to monitor process stability. The X-bar chart tracks the sample means, while the S chart tracks the sample standard deviations. The CLT ensures that these sample statistics follow predictable distributions, allowing for the establishment of control limits.</p>
<h4 id="43-financial-applications">4.3 Financial Applications</h4>
<p>In finance, the CLT is used to model returns and assess risk.</p>
<pre><code class="language-python"># Demonstrate financial applications of the CLT
def demonstrate_financial_applications():
    """
    Demonstrate financial applications of the CLT.
    """
    # Generate daily returns (log-normal distribution)
    np.random.seed(42)
    n_days = 1000
    mu = 0.0005  # Daily expected return
    sigma = 0.01  # Daily volatility

    # Generate daily returns
    daily_returns = np.random.normal(mu, sigma, n_days)

    # Calculate cumulative returns
    cumulative_returns = np.cumprod(1 + daily_returns) - 1

    # Calculate rolling means for different window sizes
    windows = [5, 20, 60]
    rolling_means = {}

    for window in windows:
        rolling_means[window] = pd.Series(daily_returns).rolling(window=window).mean()

    # Plot daily returns and cumulative returns
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # Daily returns
    ax1.plot(range(1, n_days+1), daily_returns, 'b-', alpha=0.7)
    ax1.axhline(y=mu, color='r', linestyle='--', label=f'Mean: {mu:.6f}')
    ax1.axhline(y=mu + 2*sigma, color='g', linestyle=':', label='±2σ')
    ax1.axhline(y=mu - 2*sigma, color='g', linestyle=':')

    ax1.set_xlabel('Day')
    ax1.set_ylabel('Daily Return')
    ax1.set_title('Daily Returns')
    ax1.legend()
    ax1.grid(True)

    # Cumulative returns
    ax2.plot(range(1, n_days+1), cumulative_returns, 'g-')
    ax2.set_xlabel('Day')
    ax2.set_ylabel('Cumulative Return')
    ax2.set_title('Cumulative Returns')
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig('financial_returns.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Plot rolling means
    plt.figure(figsize=(12, 8))

    for window, means in rolling_means.items():
        plt.plot(range(1, n_days+1), means, label=f'{window}-day Rolling Mean')

    plt.axhline(y=mu, color='r', linestyle='--', label=f'True Mean: {mu:.6f}')

    plt.xlabel('Day')
    plt.ylabel('Return')
    plt.title('Rolling Means of Daily Returns')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('rolling_means.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Demonstrate portfolio returns
    n_assets = 5
    n_days = 1000

    # Generate returns for multiple assets
    asset_returns = np.zeros((n_days, n_assets))
    for i in range(n_assets):
        # Different expected returns and volatilities for each asset
        mu_i = mu * (1 + 0.2 * i)
        sigma_i = sigma * (1 + 0.1 * i)
        asset_returns[:, i] = np.random.normal(mu_i, sigma_i, n_days)

    # Calculate portfolio returns with equal weights
    weights = np.ones(n_assets) / n_assets
    portfolio_returns = np.sum(asset_returns * weights, axis=1)

    # Plot individual asset returns and portfolio returns
    plt.figure(figsize=(12, 8))

    for i in range(n_assets):
        plt.plot(range(1, n_days+1), asset_returns[:, i], alpha=0.5, 
                label=f'Asset {i+1}')

    plt.plot(range(1, n_days+1), portfolio_returns, 'k-', linewidth=2, 
            label='Portfolio (Equal Weights)')

    plt.xlabel('Day')
    plt.ylabel('Return')
    plt.title('Individual Asset Returns vs. Portfolio Returns')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('portfolio_returns.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Demonstrate the CLT in portfolio returns
    sample_sizes = [5, 20, 60]
    portfolio_sample_means = {}

    for size in sample_sizes:
        n_samples = 1000
        sample_means = []

        for _ in range(n_samples):
            sample = np.random.choice(portfolio_returns, size=size, replace=True)
            sample_means.append(np.mean(sample))

        portfolio_sample_means[size] = np.array(sample_means)

    # Plot sampling distributions
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle('Sampling Distribution of Portfolio Returns', fontsize=16)

    for i, (size, means) in enumerate(portfolio_sample_means.items()):
        # Calculate theoretical parameters
        theo_mean = np.mean(portfolio_returns)
        theo_std = np.std(portfolio_returns) / np.sqrt(size)

        # Plot histogram of sample means
        sns.histplot(means, kde=True, ax=axes[i], bins=50)
        axes[i].axvline(theo_mean, color='red', linestyle='--', 
                       label=f'Theo. Mean: {theo_mean:.6f}')

        # Add normal distribution curve for comparison
        x = np.linspace(min(means), max(means), 100)
        y = stats.norm.pdf(x, theo_mean, theo_std)
        axes[i].plot(x, y * len(means) * (max(means) - min(means)) / 50, 
                    'r-', linewidth=2, label='Normal PDF')

        axes[i].set_title(f'Sample Size: {size}')
        axes[i].legend()

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig('portfolio_sampling_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()

# Demonstrate financial applications
demonstrate_financial_applications()
</code></pre>
<p><img alt="Financial Returns" src="../financial_returns.png"/>
<img alt="Rolling Means" src="../rolling_means.png"/>
<img alt="Portfolio Returns" src="../portfolio_returns.png"/>
<img alt="Portfolio Sampling Distribution" src="../portfolio_sampling_distribution.png"/></p>
<p>In finance, the CLT is used to model returns and assess risk. The demonstration shows:</p>
<ol>
<li><strong>Daily Returns</strong>: Individual daily returns may not follow a normal distribution, but their distribution becomes more normal as the time horizon increases.</li>
<li><strong>Rolling Means</strong>: As the window size increases, the rolling means become more stable and closer to the true mean.</li>
<li><strong>Portfolio Returns</strong>: The returns of a diversified portfolio tend to be more normally distributed than individual asset returns due to the CLT.</li>
<li><strong>Sampling Distribution</strong>: The sampling distribution of portfolio returns approaches a normal distribution as the sample size increases.</li>
</ol>
<h3 id="5-conclusion">5. Conclusion</h3>
<p>Through these simulations, we have observed the Central Limit Theorem in action. Key findings include:</p>
<ol>
<li>
<p><strong>Convergence to Normality</strong>: As sample size increases, the sampling distribution of the mean approaches a normal distribution, regardless of the population distribution.</p>
</li>
<li>
<p><strong>Rate of Convergence</strong>: Different distributions converge at different rates:</p>
</li>
<li>Uniform distribution converges most quickly</li>
<li>Symmetric distributions converge faster than skewed ones</li>
<li>
<p>Distributions with heavy tails require larger sample sizes</p>
</li>
<li>
<p><strong>Standard Error</strong>: The standard error of the mean decreases as the square root of the sample size increases, following the relationship <span class="arithmatex">\(\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}\)</span>.</p>
</li>
<li>
<p><strong>Practical Applications</strong>: The CLT has numerous applications in statistics, including:</p>
</li>
<li>Constructing confidence intervals</li>
<li>Quality control through control charts</li>
<li>Financial modeling and risk assessment</li>
</ol>
<p>These simulations provide an intuitive understanding of the Central Limit Theorem and its importance in statistical inference. By observing how sample means behave across different population distributions and sample sizes, we gain insight into the robustness of many statistical methods that rely on the CLT.</p>
<p>The Central Limit Theorem is truly a remarkable result that allows us to make inferences about population parameters even when the population distribution is unknown or non-normal. This makes it one of the most important theorems in statistics and a cornerstone of modern statistical practice. </p>
</div>
</div><footer>
<div aria-label="Footer Navigation" class="rst-footer-buttons" role="navigation">
<a class="btn btn-neutral float-left" href="../../5%20Circuits/Problem_1/" title="Problem 1"><span class="icon icon-circle-arrow-left"></span> Previous</a>
<a class="btn btn-neutral float-right" href="../Problem_2/" title="Estimating π using Monte Carlo Methods">Next <span class="icon icon-circle-arrow-right"></span></a>
</div>
<hr/>
<div role="contentinfo">
<!-- Copyright etc -->
</div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
</div>
</div>
</section>
</div>
<div aria-label="Versions" class="rst-versions" role="note">
<span class="rst-current-version" data-toggle="rst-current-version">
<span><a href="../../5%20Circuits/Problem_1/" style="color: #fcfcfc">« Previous</a></span>
<span><a href="../Problem_2/" style="color: #fcfcfc">Next »</a></span>
</span>
</div>
<script src="../../../js/jquery-3.6.0.min.js"></script>
<script>var base_url = "../../..";</script>
<script src="../../../js/theme_extra.js"></script>
<script src="../../../js/theme.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script src="../../../search/main.js"></script>
<script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>
</body>
</html>
